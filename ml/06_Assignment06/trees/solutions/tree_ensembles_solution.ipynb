{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ensemble methods for classification trees\n",
    "\n",
    "In this notebook you will use scikit-learn's trees and ensemble methods to compare different 'forests' and their performance on some toy datasets. In particular, you will\n",
    "\n",
    "1. train single Trees (deterministic and random ones, both greedy and random spilts) on the full data set, and compare performances.\n",
    "2. train multiple of them using bootstrapped samples of the data. compare performance vs. number of trees used.\n",
    "3. visualize some of the classifiers in 2d.\n",
    "4. train vs out ouf bag vs. test error vs. crossvalidation error.\n",
    "\n",
    "\n",
    "As usual, some setup first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.tree as sk_tree\n",
    "import sklearn.ensemble as sk_ensemble\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.cross_validation as sk_cv\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is a very simple example from scikit_learns datasets submodule. It's two dimensional, so you can visualize the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick one of these 2d datasets\n",
    "X, Y = sk_data.make_moons(2000, noise=0.1)\n",
    "#X,Y = sk_data.make_blobs(n_samples=100, n_features=2, centers=4, center_box=(-8,8))\n",
    "\n",
    "num_classes = len(np.unique(Y))\n",
    "cms = plt.cm.viridis\n",
    "\n",
    "for c in range(num_classes):\n",
    "    idx = (Y == c)\n",
    "    plt.scatter(X[idx, 0], X[idx,1], c = cms(c/(num_classes-1)), s=100)\n",
    "plt.show()\n",
    "\n",
    "n=100\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, n),\n",
    "                     np.linspace(y_min, y_max, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two convinience functions that let you plot the 'decision boundaries' of a classifiers. The first one will plot all points that would be classified the same in the same color. The second one shows a more complex picture. It overlays all classes with the alpha value determined by the fraction of the samples in a particular leaf. That means, it illustrates the confidence of a tree.\n",
    "Feel free to use any of them for the remainder of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_plot_max(trained_classifier, title, ax):\n",
    "    probs = trained_classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        tmp = np.array((np.argmax(probs, axis=1) == c).reshape(xx.shape), dtype=np.float)\n",
    "        tmp = cms(tmp)\n",
    "        ax.imshow(tmp, origin='lower', extent=[np.min(xx), np.max(xx), np.min(yy), np.max(yy)], alpha=0.8)\n",
    "        idx = (Y == c)\n",
    "        ax.scatter(X[idx, 0], X[idx,1], c = cms(c/(num_classes-1)), s=10)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "\n",
    "def classification_plot_alpha(trained_classifier, title, ax):\n",
    "    for c in range(num_classes):\n",
    "        alphas = trained_classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,c].reshape(xx.shape)\n",
    "        tmp = cms(c/(num_classes-1)*np.ones_like(xx))\n",
    "        tmp[:,:,3] = alphas\n",
    "        ax.imshow(tmp, origin='lower', extent=[np.min(xx), np.max(xx), np.min(yy), np.max(yy)], alpha=0.8)\n",
    "        idx = (Y == c)\n",
    "        ax.scatter(X[idx, 0], X[idx,1], c = cms(c/(num_classes-1)), s=10)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xlim([x_min, x_max])\n",
    "        ax.set_ylim([y_min, y_max])\n",
    "\n",
    "classification_plot = classification_plot_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some simple tree examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifiers = [(\"Decision Tree of depth  2\", sk_tree.DecisionTreeClassifier(max_depth=2)),\n",
    "               (\"Decision Tree of depth  4\", sk_tree.DecisionTreeClassifier(max_depth=4)),\n",
    "               (\"Decision Tree of depth  8\", sk_tree.DecisionTreeClassifier(max_depth=8)),\n",
    "               (\"Decision Tree of depth 16\", sk_tree.DecisionTreeClassifier(max_depth=16)),\n",
    "               (\"Extra Tree of depth  2\", sk_tree.ExtraTreeClassifier(max_depth=2)),\n",
    "               (\"Extra Tree of depth  4\", sk_tree.ExtraTreeClassifier(max_depth=4)),\n",
    "               (\"Extra Tree of depth  8\", sk_tree.ExtraTreeClassifier(max_depth=8)),\n",
    "               (\"Extra Tree of depth 16\", sk_tree.ExtraTreeClassifier(max_depth=16))\n",
    "            ]\n",
    "f, axarr = plt.subplots(len(classifiers)//2, 2, figsize=(13,21), sharey=True, sharex=True)\n",
    "\n",
    "for i, (name, classifier) in enumerate(classifiers):\n",
    "    classifier.fit(X,Y)\n",
    "    classification_plot(classifier, name, axarr[i//2, i%2])\n",
    "\n",
    "f.subplots_adjust(hspace=0.2, wspace=0.05)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to draw similar pictures for the following classifiers:\n",
    "1. bagged classification trees\n",
    "2. random forests\n",
    "3. extra trees\n",
    "\n",
    "All are accessible in scikit learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert code below\n",
    "classifiers = [(\"Bagging  4 DTs\", sk_ensemble.BaggingClassifier(sk_tree.DecisionTreeClassifier(), n_estimators=4)),\n",
    "               (\"Bagging 8 DTs\", sk_ensemble.BaggingClassifier(sk_tree.DecisionTreeClassifier(), n_estimators=8)),\n",
    "               (\"Bagged 16 DTs\", sk_ensemble.BaggingClassifier(sk_tree.DecisionTreeClassifier(), n_estimators=16)),\n",
    "               (\"Bagged 32 DTs\", sk_ensemble.BaggingClassifier(sk_tree.DecisionTreeClassifier(), n_estimators=32)),         \n",
    "               (\"Random Forest with 4 Trees\", sk_ensemble.RandomForestClassifier(n_estimators=4)),\n",
    "               (\"Random Forest with 8 Trees\", sk_ensemble.RandomForestClassifier(n_estimators=8)),\n",
    "               (\"Random Forest with 16 Trees\", sk_ensemble.RandomForestClassifier(n_estimators=16)),\n",
    "               (\"Random Forest with 32 Trees\", sk_ensemble.RandomForestClassifier(n_estimators=32)),\n",
    "               (\" 4 Extra Trees\", sk_ensemble.ExtraTreesClassifier(n_estimators=4)),\n",
    "               (\" 8 Extra Trees\", sk_ensemble.ExtraTreesClassifier(n_estimators=8)),\n",
    "               (\"16 Extra Trees\", sk_ensemble.ExtraTreesClassifier(n_estimators=16)),\n",
    "               (\"32 Extra Trees\", sk_ensemble.ExtraTreesClassifier(n_estimators=32)),\n",
    "\n",
    "            ]\n",
    "\n",
    "f, axarr = plt.subplots(len(classifiers)//2, 2, figsize=(13,31), sharey=True, sharex=True)\n",
    "\n",
    "for i, (name, classifier) in enumerate(classifiers):\n",
    "    classifier.fit(X,Y)\n",
    "    classification_plot(classifier, name, axarr[i//2, i%2])\n",
    "\n",
    "f.subplots_adjust(hspace=0.2, wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the ensemble methods generalize and how the out of bag error estimates the validation error. To make things a bit more interesting, we shall use a higher dimensional problem with more classes and more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = sk_data.make_classification(10000, n_features=20, n_classes = 4, n_informative = 4)\n",
    "X_train, X_test, Y_train, Y_test = sk_cv.train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "num_trees = range(1,65,2)\n",
    "accuracies = []\n",
    "for nt in num_trees:\n",
    "    # Sometimes not every input point was 'out of bag' and scikit learn raises a warning when computing the OOB-score.\n",
    "    # This just supresses this warning to unclutter the notebook.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        rf = sk_ensemble.RandomForestClassifier(n_estimators=nt, bootstrap=True, oob_score=True)\n",
    "        rf.fit(X_train, Y_train)\n",
    "    accuracies.append((rf.score(X_train, Y_train), rf.score(X_test, Y_test), rf.oob_score_))\n",
    "accuracies = np.array(accuracies)\n",
    "plt.plot(num_trees, accuracies[:,0], label='train accuracy', linewidth=2)\n",
    "plt.plot(num_trees, accuracies[:,1], label='test accuracy', linewidth=2)\n",
    "plt.plot(num_trees, accuracies[:,2], label='OOB accuracy', linewidth=2)\n",
    "plt.ylim([0.5,1.05])\n",
    "plt.title(\"Train, test and out-of-bag error of a Random Forest\")\n",
    "plt.xlabel(\"Number of trees in the forest\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid('on','both')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "What are advantages/disadvantages of using the out-of-bag error rather than the 'traditional' train/test split?\n",
    "\n",
    "**Answers**\n",
    "\n",
    "Advantages of out-of-bag error over a train/test split:\n",
    "\n",
    "  * little overhead to compute\n",
    "  * no need for an additional validation set\n",
    "  * ...\n",
    "  \n",
    "Disadvantages of out-of-bag error over a train/test split:\n",
    "\n",
    "  * unreliable with only a few trees\n",
    "  * can not be computed for every ml-model (requires bootstrap aggregation)\n",
    "  * ...\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this to a 'real' dataset and investigate some more. In the cell below, the iris data set is loaded. Add code to compute:\n",
    "1. the training error\n",
    "2. the test error\n",
    "3. the out of bag error\n",
    "4. a crossvalidation estimate of the error using 4-fold CV\n",
    "\n",
    "for different numbers of trees.\n",
    "\n",
    "Create plots similar to the one above for a random forests and extra trees. What do you observe?\n",
    "\n",
    "**Answer**: \n",
    "  * OOB underestimates accuracy with only a few trees\n",
    "  * training accuracy quickly goes to 1, while other metricy never achieve an accuracy of 1\n",
    "  * OOB accuracy and CV accuracy yield similar values (as they both work only on the training set) in contrast to test accuracy\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Iris data from the first exercise sheet\n",
    "X_train = np.loadtxt('iris_train.data', delimiter=' ', dtype=float)\n",
    "Y_train = np.loadtxt('iris_train.labels', dtype=int)\n",
    "X_test = np.loadtxt('iris_test.data', delimiter=' ', dtype=float)\n",
    "Y_test = np.loadtxt('iris_test.labels', dtype=int)\n",
    "\n",
    "# Insert code below!\n",
    "\n",
    "num_trees = range(1,32,2)\n",
    "accuracies = []\n",
    "for nt in num_trees:\n",
    "    # Sometimes not every input point was 'out of bag' and scikit learn raises a warning when computing the OOB-score.\n",
    "    # This just supresses this warning to unclutter the notebook.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        rf = sk_ensemble.RandomForestClassifier(n_estimators=nt, bootstrap=True, oob_score=True, random_state=None)\n",
    "        rf.fit(X_train, Y_train)\n",
    "        train_acc = rf.score(X_train, Y_train)\n",
    "        test_acc = rf.score(X_test, Y_test)\n",
    "        oob = rf.oob_score_\n",
    "        cv_score = sk_cv.cross_val_score(rf, X_train, Y_train, cv=4).mean()\n",
    "    accuracies.append((train_acc, test_acc, oob, cv_score))\n",
    "    \n",
    "accuracies_rf = np.array(accuracies)\n",
    "\n",
    "accuracies = []\n",
    "for nt in num_trees:\n",
    "    # Supress warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ets = sk_ensemble.ExtraTreesClassifier(n_estimators=nt, bootstrap=True, oob_score=True, random_state=None)\n",
    "        ets.fit(X_train, Y_train)\n",
    "        train_acc = ets.score(X_train, Y_train)\n",
    "        test_acc = ets.score(X_test, Y_test)\n",
    "        oob = ets.oob_score_\n",
    "        cv_score = sk_cv.cross_val_score(ets, X_train, Y_train, cv=4).mean()\n",
    "    accuracies.append((train_acc, test_acc, oob, cv_score))\n",
    "\n",
    "\n",
    "accuracies_ets = np.array(accuracies)\n",
    "\n",
    "\n",
    "f, (rf_ax, ets_ax) = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "rf_ax.plot(num_trees, accuracies_rf[:,0], label='train accuracy', linewidth=2)\n",
    "rf_ax.plot(num_trees, accuracies_rf[:,1], label='test accuracy', linewidth=2)\n",
    "rf_ax.plot(num_trees, accuracies_rf[:,2], label='OOB accuracy', linewidth=2)\n",
    "rf_ax.plot(num_trees, accuracies_rf[:,3], label='CV accuracy', linewidth=2)\n",
    "rf_ax.set_ylim([0.5,1.05])\n",
    "rf_ax.set_title(\"Train, test, CV and out-of-bag error of a Random Forest\")\n",
    "rf_ax.set_xlabel(\"Number of trees in the forest\")\n",
    "rf_ax.set_ylabel(\"Accuracy\")\n",
    "rf_ax.grid('on','both')\n",
    "rf_ax.legend(loc=4)\n",
    "\n",
    "ets_ax.plot(num_trees, accuracies_ets[:,0], label='train accuracy', linewidth=2)\n",
    "ets_ax.plot(num_trees, accuracies_ets[:,1], label='test accuracy', linewidth=2)\n",
    "ets_ax.plot(num_trees, accuracies_ets[:,2], label='OOB accuracy', linewidth=2)\n",
    "ets_ax.plot(num_trees, accuracies_ets[:,3], label='CV accuracy', linewidth=2)\n",
    "ets_ax.set_ylim([0.5,1.05])\n",
    "ets_ax.set_title(\"Train, test, CV and out-of-bag error of ExtraTrees\")\n",
    "ets_ax.set_xlabel(\"Number of trees in the forest\")\n",
    "ets_ax.set_ylabel(\"Accuracy\")\n",
    "ets_ax.grid('on','both')\n",
    "ets_ax.legend(loc=4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
