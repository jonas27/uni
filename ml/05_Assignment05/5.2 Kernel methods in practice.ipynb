{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Kernel methods in practice\n",
    "In the current exercise, we will apply the concepts learned about kernel-methods using 4 toy datasets : moons, circles, blobs, and swiss roll. \n",
    "\n",
    "__Task__: Using the kernels available in the support vector machine (SVM) implementation found in scikit learn (linear, polynomial, Gaussian, or sigmoid), find the kernel and the corresponding parameter for which SVM achieves the best classification performance. Specifically, experiment with the value of the $\\gamma$ parameter, which controls the influence of each of the training samples in the kernel-projection, and therefore, the bias introduced by the kernel used. For further information, see the [scikit learning documentation](http://scikit-learn.org/stable/modules/svm.html#kernel-functions) about the kernels used. Leave the other parameters, as the regularization strength $C$ and the degree of the polynomial kernel fixed.\n",
    "\n",
    "__Hint 1__: Implement a grid search to find the optimal $\\gamma$ value for each of the kernels.\n",
    "\n",
    "__Hint 2__: Don't forget to use an adequate crossvalidation framework to avoid overfitting.\n",
    "\n",
    "__Q 5.2.1__ For each of the datasets, what is the kernel with which SVM achieves the best classification performance? What is the corresponding $\\gamma$ value?\n",
    "\n",
    "__Task__: For each of the kernels studied above, find the corresponding K-PCA projection using the same kernel parameters. Visualize the achieved projection and compare the result with projects resulting from using different $\\gamma$ values.\n",
    "\n",
    "__Q 5.2.2__ For each of the datasets, what is the kernel with which the data has the best separability (visual inspection)? What is the corresponding $\\gamma$ value? Does the kernel and the $\\gamma$ values correspond to the ones for which the performance on __Q 5.2.1__ was the highest?\n",
    "\n",
    "__Q 5.2.3__ How many components yield K-PCA? How and why is it different from the number of components delivered by PCA?\n",
    "\n",
    "__BONUS__: Scikit learn provides a method to automatically perform a grid search and find the optimal parameters for a given kernel when training a SVM. Find and use the mentioned method to find the optimal set of parameters for each of the Kernels and datasets. Are the $\\gamma$ values the you found for __Q 5.2.1__  and __Q 5.2.2__  similar to the ones yield by the automatic grid search function provided by scikit learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn import svm\n",
    "\n",
    "def scatter_plotDataset(fig,x,y):\n",
    "    colors = ['r','b']   \n",
    "    if x.shape[1] == 2:        \n",
    "        ax = fig.add_subplot(111)\n",
    "        for idx_class in range(2):\n",
    "            ax.scatter(x[y==idx_class,0],x[y==idx_class,1],\n",
    "                     c=colors[idx_class])\n",
    "    elif x.shape[1] == 3:        \n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        for idx_class in range(2):\n",
    "            cclass = y==idx_class\n",
    "            ax.scatter(x[cclass,0],x[cclass,1],x[cclass,2],\n",
    "                     c=colors[idx_class])\n",
    "            ax.set_zlabel('feat. 3')\n",
    "    ax.set_xlabel('feat. 1')\n",
    "    ax.set_ylabel('feat. 2')\n",
    "    return ax\n",
    "\n",
    "# load data\n",
    "# Note: The swiss_roll dataset is significantly more computation-intensive than the others.\n",
    "# You can comment the line below out to speed things up, while you test your approach\n",
    "datasets = {'moons': [],\n",
    "           'circles': [],\n",
    "           'blobs': [],\n",
    "           'swiss_roll': [], # comment this line out for faster processing\n",
    "           }\n",
    "for key, value in datasets.items():\n",
    "    fname = os.path.join('datasets',key+'.data')\n",
    "    x = np.loadtxt(fname)\n",
    "    fname = os.path.join('datasets',key+'.labels')\n",
    "    y = np.loadtxt(fname)\n",
    "    fig = plt.figure()\n",
    "    plt.grid()\n",
    "    ax = scatter_plotDataset(fig,x,y)\n",
    "    ax.set_title(key)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
