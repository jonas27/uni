{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN clustering\n",
    "\n",
    "One distinct disadvantage of k-means clustering approaches is the requirement to know $k$ before actually clustering. Oftentimes, the number of clusters is not known beforehand and an extensive evaluation for all possible values of $k$ requires intensive computation times. The presented density-based DBSCAN method circumvents this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.rc('image', cmap='nipy_spectral')\n",
    "X = np.loadtxt('data/shapes.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Evaluate the k-means algorithm on the provided `shapes` data set and plot its results. To choose suitable values $k$, evaluate the average silhouette score for values for $k \\in [2, 20]$. How would you rate the resulting cluster assignments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as defined in 8.1\n",
    "def plot_clustering(data, labels, centroids=None, title='', ax=None):\n",
    "    if data.shape[1] != 2:\n",
    "        raise ValueError('Only two-dimensional data is supported.')\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1,1)\n",
    "    ax.set_title(title)\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=labels)\n",
    "    if centroids is not None:\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], c='magenta', s=150, marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "# Your code below here\n",
    "Ks = list(range(2,21))\n",
    "for K in Ks:\n",
    "    np.random.seed(42)\n",
    "    kmeans = KMeans(n_clusters=K).fit(X)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    score = silhouette_score(X, cluster_labels)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(Ks, scores)\n",
    "plt.xlabel('K clusters')\n",
    "plt.ylabel('Average silhouette score')\n",
    "plt.title('Silhouette score for $K$ clusters')\n",
    "\n",
    "# We see that 3 or 4 or maybe 6 cluster seem sensible according to silhouette score\n",
    "\n",
    "good_Ks = [3, 4, 6]\n",
    "for K in good_Ks:\n",
    "    np.random.seed(42)  # ensure reproducible results\n",
    "    kmeans = KMeans(n_clusters=K).fit(X)\n",
    "    plot_clustering(X, kmeans.labels_, centroids=kmeans.cluster_centers_, title=f'$K={K}$, silhouette score: {scores[K-2]:1.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Judging which clustering makes most sense is not easy, e.g. should the dumbell-like shape on the right be one or two clusters? Generally, the results of k-means clustering seem not appropriate for this data set as blobs of data are often split and the spurious individual data points are all assigned to a specific cluster.\n",
    "\n",
    "#### Task 2\n",
    "\n",
    "a) Use the `DBSCAN` algorithm provided by sklearn to calculate a new clustering of the data. Plot the results in a scatter plot colored by cluster label. Which clustering result do you prefer, one of the k-means runs or the DBSCAN clustering?\n",
    "\n",
    "b) Calculate the silhouette score for the DBSCAN clustering result, compare them and explain the difference. Which part of the silhouette score formula is presumably responsible for this difference?\n",
    "\n",
    "*Note:* DBSCAN produces also an outlier cluster with the label `-1`. Using the `nipy_spectral` colormap, data points belonging to this cluster are always plotted in black. For calculation of the silhouette score, do not consider the samples contained in the outlier cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below here\n",
    "dbscan = DBSCAN().fit(X)\n",
    "select_idx = dbscan.labels_ >= 0\n",
    "X_no_outliers = X[select_idx, :]\n",
    "labels_no_outliers = dbscan.labels_[select_idx]\n",
    "score = silhouette_score(X_no_outliers, labels_no_outliers)\n",
    "plot_clustering(X, dbscan.labels_, title=f'DBSCAN, silhouette score: {score:1.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "a) The DBSCAN algorithm produces clusters that make more sense, as connected data is put into one cluster. Spurious data points are considered outliers.\n",
    "\n",
    "b) The DBSCAN silhouette score is lower than that of k-means. For this data set the average silhouette score of all data points is not a sensible metric to compare clustering results, as within cluster similarity being defined by average distance to cluster members is not supported by clusters that are two connected blobs. This is expressed as $a(i)$ in the formula for the silhouette score.\n",
    "\n",
    "#### Task 3\n",
    "\n",
    "Run the DBSCAN on the following modified versions of the shapes data set `X_1` and `X_2`. Plot the clustering results. Explain the results and try to modify DBSCAN's hyperparameters in such a way that it produces more sensible results for the modified datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X*0.4\n",
    "X_2 = X*2\n",
    "\n",
    "# Your code below here\n",
    "\n",
    "dbscan = DBSCAN().fit(X_1)\n",
    "plot_clustering(X_1, dbscan.labels_, title=f'DBSCAN for X_1')\n",
    "\n",
    "dbscan = DBSCAN().fit(X_2)\n",
    "plot_clustering(X_2, dbscan.labels_, title=f'DBSCAN for X_2')\n",
    "\n",
    "# as the 'new' data sets are only rescaled versions of the original data set,\n",
    "# we simply need to adjust the density parameter eps\n",
    "\n",
    "dbscan = DBSCAN(eps=.5*.4).fit(X_1)\n",
    "plot_clustering(X_1, dbscan.labels_, title=f'Density corrected DBSCAN for X_1')\n",
    "\n",
    "dbscan = DBSCAN(eps=.5*2).fit(X_2)\n",
    "plot_clustering(X_2, dbscan.labels_, title=f'Density corrected DBSCAN for X_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette score for each data point individually\n",
    "\n",
    "As we learned previously, the silhouette score can be calculated for each sample individually while we so far only used the average value.\n",
    "\n",
    "The provided function `plot_silhouette_samples` visualizes the silhouette score for each data point individually. This function sorts the silhouette scores for each data point contained in a cluster in a descending fashion The red line indicates the average silhouette score of all data points across clusters. Note that this function discards outlier clusters, as these distort silhouette scores if considered as a regular cluster.\n",
    "\n",
    "#### Task 4\n",
    "\n",
    "Plot and interpret the results of using DBSCAN with default `eps=0.5` on data set `X` and compare it with silhouette scores obtained using k-means ($k=4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_silhouette_samples(data, labels, title=''):\n",
    "    idx_no_outliers = labels >= 0\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "    plot_clustering(data, labels, title='Clustering ' + title, ax=axes[0])\n",
    "    # discard outliers for calculation of silhouette scores\n",
    "    outliers_present = np.min(labels) == -1\n",
    "    data = data[idx_no_outliers, :]\n",
    "    labels = labels[idx_no_outliers]\n",
    "    avg_score = silhouette_score(data, labels)\n",
    "    ax = axes[1]\n",
    "    ax.axvline(avg_score, linestyle='--', c='r')\n",
    "    n_clusters = np.max(labels)\n",
    "    sample_score = silhouette_samples(data, labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters+1):\n",
    "        ith_cluster_sample_score = \\\n",
    "            sample_score[labels == i]\n",
    "\n",
    "        ith_cluster_sample_score.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_sample_score.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i+int(outliers_present)) /\n",
    "                                 (n_clusters+int(outliers_present)))\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_sample_score,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 1  # 10 for the 0 samples\n",
    "    ax.set_title('Silhouette samples ' + title)\n",
    "    ax.set_xlim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below here\n",
    "dbscan = DBSCAN().fit(X)\n",
    "np.random.seed(42)\n",
    "kmeans = KMeans(n_clusters=4).fit(X)\n",
    "plot_silhouette_samples(X, dbscan.labels_, title='DBSCAN')\n",
    "plot_silhouette_samples(X, kmeans.labels_, title='KMeans, $K=4$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "The small dense blobs of data are represented much better using the DBSCAN results. However especially the two clusters made up of connected blobs of data produce bad silhouette scores. The samples that produce negative low values for $s(i)$ can be accounted to the connected blobs. \n",
    "Intuitively put, the shapes of the data clusters in this data set do not fit well to the Euclidean distance metric used in the silhouette score. \n",
    "\n",
    "### Notes\n",
    "\n",
    "Data sets in assignment 10 were taken from [1] and slightly modified.\n",
    "\n",
    "[1] P. Fränti and S. Sieranoja\n",
    "K-means properties on six clustering benchmark datasets\n",
    "Applied Intelligence, 48 (12), 4743-4759, December 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
